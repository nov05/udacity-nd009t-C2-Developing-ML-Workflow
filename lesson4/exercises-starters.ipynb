{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* changed by nov05 on 2024-11-24  \n",
    "* Udacity content: [exercise](https://www.evernote.com/shard/s139/u/0/sh/d22b9fe5-9992-4dd0-9402-c623cdbc90b4/rJcBRGAXxAQfdkl3kqgZ1N2VIKFVDSOLDimPDJFMhwHEQmyRu0AHwQTqxw), [solution](https://www.evernote.com/shard/s139/u/0/sh/00654bdd-0c00-4525-a0e4-beeeccb17e18/UdvEWior2s6PYMczVqGFVPf1PM_g35bFRrPiVlm79SfmnVhyPY_BJQPsiw)    \n",
    "* my notes:\n",
    "  * [Feature Store](https://docs.google.com/document/d/1pZtRTDmZYiaHO4kFcGYEDSneVZ98klFH8E4YR2hPUAk)  \n",
    "  * [Data quanlity and Clarify model monitors](https://docs.google.com/document/d/1ICJvBqJqBtDuTwTkyB10egC20sBSqFtdWq7zk0BEuQA)   \n",
    "\n",
    "* Tips: \n",
    "  * I was able to run this notebook on my local computer to train and deploy model, create monitors. \n",
    "  * I would have been able to create Feature Store group as well if the assumed role had the permissions. \n",
    "  * To run SageMaker in the local environment, add the following 3 keys to `~/.aws/credentials`.\n",
    "    ```bash  \n",
    "    (awsmle_py310) PS D:\\github\\udacity-nd009t-C2-Developing-ML-Workflow> cat ~/.aws/credentials\n",
    "    [default]\n",
    "    aws_access_key_id = *KDV4\n",
    "    aws_secret_access_key = *U4jB\n",
    "    aws_session_token = *vQ==    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "This is the notebook containing the exercises for **Feature Store**, **Model Monitor**, and **Clarify**. Tested for these exercises was performed using __2 vCPU + 4 GiB notebook instance with Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized) kernel__.\n",
    "\n",
    "## Staging\n",
    "\n",
    "We'll begin by initializing some variables. These are often assumed to be present in code samples you'll find in the AWS documenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\pydantic\\_internal\\_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 18:11:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 18:11:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=883390;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=919137;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\guido\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 18:11:32] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 18:11:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=932782;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=318265;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 18:11:34] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Couldn't call <span style=\"color: #008700; text-decoration-color: #008700\">'get_role'</span> to get Role ARN from role name voclabs to get <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#5902\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5902</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Role path.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 18:11:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Couldn't call \u001b[38;2;0;135;0m'get_role'\u001b[0m to get Role ARN from role name voclabs to get \u001b]8;id=338138;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=461607;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#5902\u001b\\\u001b[2m5902\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         Role path.                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role ARN: arn:aws:iam::807711953667:role/service-role/AmazonSageMaker-ExecutionRole-20241121T213663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=439693;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=390344;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\botocore\\credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sagemaker # type: ignore\n",
    "from sagemaker import get_execution_role # type: ignore\n",
    "from sagemaker.session import Session # type: ignore\n",
    "\n",
    "role_arn = get_execution_role()  ## get role ARN\n",
    "if 'AmazonSageMaker-ExecutionRole' not in role_arn:\n",
    "    role_arn = \"arn:aws:iam::807711953667:role/service-role/AmazonSageMaker-ExecutionRole-20241121T213663\"\n",
    "print(\"Role ARN:\", role_arn) ## If local, Role ARN: arn:aws:iam::807711953667:role/voclabs\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "# bucket = session.default_bucket()\n",
    "bucket = \"sagemaker-studio-807711953667-mmx0am1bt28\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **👉 Feature Store**  \n",
    "---\n",
    "\n",
    "* Check [the Google Docs notes](https://docs.google.com/document/d/1pZtRTDmZYiaHO4kFcGYEDSneVZ98klFH8E4YR2hPUAk)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Store is a special database to give ML systems a consistent data flow across training and inference workloads. It can ingest data in batches (for training) as well as serve input features to models with very low latency for real-time prediction.\n",
    "\n",
    "For this exercise we'll work with a wine quality dataset: https://archive.ics.uci.edu/ml/datasets/wine+quality/\n",
    "\n",
    "```\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
      "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
      "       'proanthocyanins', 'color_intensity', 'hue',\n",
      "       'od280/od315_of_diluted_wines', 'proline'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from sklearn import datasets # type: ignore\n",
    "import time\n",
    "# import uuid\n",
    "\n",
    "data = datasets.load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we leave the column names as-is, Feature Store won't be able to handle the `/` in `od280/od315_of_diluted_wines` (`/` is a delimiter Feature Store uses to manage how features are organized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>EventTime</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1.732480e+09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>12.58</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.55</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1.732480e+09</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680.0</td>\n",
       "      <td>1.732480e+09</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "154    12.58        1.29  2.10               20.0      103.0           1.48   \n",
       "161    13.69        3.26  2.54               20.0      107.0           1.83   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "154        0.58                  0.53             1.40             7.60  0.58   \n",
       "161        0.56                  0.50             0.80             5.88  0.96   \n",
       "\n",
       "     od280_od315_of_diluted_wines  proline     EventTime   ID  \n",
       "4                            2.93    735.0  1.732480e+09    4  \n",
       "154                          1.55    640.0  1.732480e+09  154  \n",
       "161                          1.82    680.0  1.732480e+09  161  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)\n",
    "## Add columns for feature group\n",
    "df[\"EventTime\"] = time.time()\n",
    "df[\"ID\"] = range(len(df))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data, we can create a feature group. Remember to attach event time and ID columns - Feature Store needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='alcohol', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='malic_acid', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='ash', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='alcalinity_of_ash', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='magnesium', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='total_phenols', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='flavanoids', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='nonflavanoid_phenols', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='proanthocyanins', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='color_intensity', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='hue', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='od280_od315_of_diluted_wines', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='proline', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='EventTime', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='ID', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup # type: ignore\n",
    " \n",
    "# TODO: Create feature group\n",
    "feature_group_name = \"wine-features\"\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name, \n",
    "    sagemaker_session=session\n",
    ")\n",
    "# TODO: Load Feature definitions\n",
    "feature_group.load_feature_definitions(data_frame=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature group is not created until we call the `create` method, let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:807711953667:feature-group/wine-features',\n",
       " 'ResponseMetadata': {'RequestId': '9adc9df7-1db7-4c01-85ea-063dc7aa4f9d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9adc9df7-1db7-4c01-85ea-063dc7aa4f9d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Sun, 24 Nov 2024 18:28:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the feature store:\n",
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/features\",\n",
    "    record_identifier_name='ID',\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role_arn,\n",
    "    enable_online_store=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🟢⚠️ Issue solved: I got the following response when creating from a local notebook. However, the creation failed. Go to `SageMaker Studio > Data > Feature Store`. Click on the feature group. Click on the “Details” tab. It seems the assumed role doesn’t have certain permissions. While creating from a SageMaker notebook, it succeeded. Data is written in Parquet format with partitions to AWS S3.      \n",
    "```\n",
    "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:807711953667:feature-group/wine-features',\n",
    " 'ResponseMetadata': {'RequestId': 'd49e3486-e1cb-414e-86fe-0f56d5fbf5fa',\n",
    "  'HTTPStatusCode': 200,\n",
    "  'HTTPHeaders': {'x-amzn-requestid': 'd49e3486-e1cb-414e-86fe-0f56d5fbf5fa',\n",
    "   'content-type': 'application/x-amz-json-1.1',\n",
    "   'content-length': '90',\n",
    "   'date': 'Sun, 24 Nov 2024 17:56:18 GMT'},\n",
    "  'RetryAttempts': 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "feature_group = FeatureGroup(name=feature_group_name)\n",
    "feature_group_status = feature_group.describe()['FeatureGroupStatus']\n",
    "print(feature_group_status, type(feature_group_status))  ## CreateFailed <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, ingest some data into your feature group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "if feature_group_status=='Created':\n",
    "    feature_group.ingest(data_frame=df, \n",
    "                        max_workers=5, \n",
    "                        wait=True)\n",
    "else:\n",
    "    print('⚠️ The feature group is not created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You've demonstrated your understanding of creating feature groups and ingesting data into them using Feature Store. Next up we'll cover Model Monitor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **👉 Model Monitor**    \n",
    "\n",
    "---   \n",
    "\n",
    "* Check [the Google Docs notes](https://docs.google.com/document/d/1ICJvBqJqBtDuTwTkyB10egC20sBSqFtdWq7zk0BEuQA)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll create a monitoring schedule for a deployed model. We're going to provide code to help you deploy a model and get started, so that you can focus on Model Monitor for this exercise. __Remember to clean up your model before you end a work session__. We'll provide some code at the end to help you clean up your model. We'll begin by reloading our data from the previous exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from sklearn import datasets # type: ignore\n",
    "\n",
    "data = datasets.load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "df.rename(columns = {'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to put the target variable in the first column per the docs for our chosen algorithm: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html  \n",
    "\n",
    "* Tips:   \n",
    "  * Ensuring the 'TARGET' Column is the First Column:  \n",
    "  *After the df.reset_index(inplace=True), the 'TARGET' column becomes the first column in the DataFrame. If the intention was to reorder the DataFrame columns, this sequence achieves that by popping the 'TARGET' column, making it the index, and then converting it back to a regular column (now at the front).*\n",
    "  * Removing the Column Before Resetting It:  \n",
    "  *df.pop('TARGET') removes 'TARGET' from the DataFrame, ensuring it is not duplicated when resetting the index (as reset_index() would otherwise add the index back as a new column).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2</td>\n",
       "      <td>12.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.64</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.14</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.73</td>\n",
       "      <td>880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>16.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.59</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TARGET  alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\n",
       "14        0    14.38        1.87  2.38               12.0      102.0   \n",
       "157       2    12.45        3.03  2.64               27.0       97.0   \n",
       "61        1    12.64        1.36  2.02               16.8      100.0   \n",
       "\n",
       "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "14            3.30        3.64                  0.29             2.96   \n",
       "157           1.90        0.58                  0.63             1.14   \n",
       "61            2.02        1.41                  0.53             0.62   \n",
       "\n",
       "     color_intensity   hue  od280_od315_of_diluted_wines  proline  \n",
       "14              7.50  1.20                          3.00   1547.0  \n",
       "157             7.50  0.67                          1.73    880.0  \n",
       "61              5.75  0.98                          1.59    450.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"TARGET\"] = data['target']\n",
    "df.set_index(df.pop('TARGET'), inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the data to S3 as train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-studio-807711953667-mmx0am1bt28/wine/data/train.csv\n",
      "s3://sagemaker-studio-807711953667-mmx0am1bt28/wine/data/validation.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import BytesIO #StringIO\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "delimiter = int(len(df)/2)\n",
    "df_train, df_val = df.iloc[delimiter:], df.iloc[:delimiter]\n",
    "\n",
    "## prepare training data \n",
    "csv_buffer = BytesIO()\n",
    "df_train.to_csv(csv_buffer, header=False, index=False)  ## send to buffer\n",
    "csv_buffer.seek(0)\n",
    "s3_key = \"wine/data/train.csv\"  \n",
    "s3_client.put_object(Body=csv_buffer, Bucket=bucket, Key=s3_key)  ## uplaod to S3\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{s3_key}\", \n",
    "    content_type='csv')\n",
    "print(f\"s3://{bucket}/{s3_key}\") \n",
    "\n",
    "## prepare validation data\n",
    "csv_buffer = BytesIO()\n",
    "df_val.to_csv(csv_buffer, header=False, index=False)  ## send to buffer\n",
    "csv_buffer.seek(0)\n",
    "s3_key = \"wine/data/validation.csv\" \n",
    "s3_client.put_object(Body=csv_buffer, Bucket=bucket, Key=s3_key)  ## upload to S3\n",
    "val_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{s3_key}\", \n",
    "    content_type='csv')\n",
    "print(f\"s3://{bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 19:39:20] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 19:39:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=13996;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=387960;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\u001b\\\u001b[2m523\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name: xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-39-20-779       <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name: xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-39-20-779       \u001b]8;id=674815;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=316261;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-25 01:39:27 Starting - Starting the training job...\n",
      "2024-11-25 01:39:50 Starting - Preparing the instances for training......\n",
      "2024-11-25 01:40:45 Downloading - Downloading input data...\n",
      "2024-11-25 01:41:10 Downloading - Downloading the training image.....Arguments: train\n",
      "[2024-11-25:01:42:27:INFO] Running standalone xgboost training.\n",
      "[2024-11-25:01:42:27:INFO] File size need to be processed in the node: 0.01mb. Available memory size in the node: 8456.52mb\n",
      "[2024-11-25:01:42:27:INFO] Determined delimiter of CSV input is ','\n",
      "[01:42:27] S3DistributionType set as FullyReplicated\n",
      "[01:42:27] 89x13 matrix with 1157 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\n",
      "[2024-11-25:01:42:27:INFO] Determined delimiter of CSV input is ','\n",
      "[01:42:27] S3DistributionType set as FullyReplicated\n",
      "[01:42:27] 89x13 matrix with 1157 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[0]#011train-rmse:0.9357#011validation-rmse:0.53422\n",
      "Multiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\n",
      "Will train until validation-rmse hasn't improved in 10 rounds.\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 2 pruned nodes, max_depth=1\n",
      "[1]#011train-rmse:0.759657#011validation-rmse:0.685354\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[2]#011train-rmse:0.616137#011validation-rmse:0.821641\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 2 pruned nodes, max_depth=1\n",
      "[3]#011train-rmse:0.501087#011validation-rmse:0.858446\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\n",
      "[4]#011train-rmse:0.430041#011validation-rmse:0.925116\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\n",
      "[5]#011train-rmse:0.377443#011validation-rmse:0.979923\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 4 pruned nodes, max_depth=0\n",
      "[6]#011train-rmse:0.337549#011validation-rmse:1.0265\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\n",
      "[7]#011train-rmse:0.309158#011validation-rmse:1.06425\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\n",
      "[8]#011train-rmse:0.287472#011validation-rmse:1.09787\n",
      "[9]#011train-rmse:0.273879#011validation-rmse:1.12296\n",
      "[10]#011train-rmse:0.264732#011validation-rmse:1.1432\n",
      "Stopping. Best iteration:\n",
      "[0]#011train-rmse:0.9357#011validation-rmse:0.53422\n",
      "[01:42:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\n",
      "\n",
      "2024-11-25 01:42:46 Training - Training image download completed. Training in progress.\n",
      "2024-11-25 01:42:46 Uploading - Uploading generated training model\n",
      "2024-11-25 01:42:46 Completed - Training job completed\n",
      "Training seconds: 121\n",
      "Billable seconds: 121\n",
      "CPU times: total: 4.05 s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "algo_image = sagemaker.image_uris.retrieve(\"xgboost\", region, version='latest')\n",
    "s3_model_output = f\"s3://{bucket}/wine/model\"\n",
    "\n",
    "model=sagemaker.estimator.Estimator(\n",
    "    image_uri=algo_image,\n",
    "    role=role_arn,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_model_output,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "model.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='reg:linear',\n",
    "    early_stopping_rounds=10,\n",
    "    num_round=200\n",
    ")\n",
    "model.fit({\n",
    "    'train': train_input, \n",
    "    'validation': val_input\n",
    "})\n",
    "## go to \"SageMaker - Training - Training jobs\". Make sure the job is completed.\n",
    "## CPU times: total: 4.05 s\n",
    "## Wall time: 3min 33s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your training job has finished, you can perform the first task in this exercise:   \n",
    "* Creating a data capture config. Configure your model to sample `34%` of inferences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 83.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO\n",
    "from sagemaker.model_monitor import DataCaptureConfig # type: ignore\n",
    "\n",
    "destination_s3_uri = f's3://{bucket}/wine/data-capture'\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=34,\n",
    "    destination_s3_uri=destination_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We'll use your config to deploy a model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 19:48:49] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-48-48-543              <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 19:48:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-48-48-543              \u001b]8;id=941270;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=366574;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4025\u001b\\\u001b[2m4025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-48-48-543     <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#5820\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5820</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-48-48-543     \u001b]8;id=713305;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=580388;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#5820\u001b\\\u001b[2m5820\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 19:48:50] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-48-48-543            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4642\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4642</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 19:48:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-48-48-543            \u001b]8;id=404575;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=458316;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4642\u001b\\\u001b[2m4642\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_predictor = model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")\n",
    "## go to \"SageMaker - Inference - Endpoints\" to check the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You should see an indicator like this when the deployment finishes:\n",
    "\n",
    "```\n",
    "-----------------!\n",
    "```\n",
    "We can test your deployment like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6030303239822388,0.6030303239822388,0.6030303239822388,0.6030303239822388,0.6030303239822388'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "x_pred = xgb_predictor.predict(\n",
    "    df_val.drop(columns=df_val.columns[0]).sample(5).values  ## Drop the target column\n",
    ").decode('utf-8')\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All systems go! To finish up the exercise, we're going to provide you with a `DefaultModelMonitor` and a suggested baseline. Combine the `xgb_predictor` and the provided `my_monitor` to configure the monitoring schedule for _hourly_ monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:00:29] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to the only supported framework/algorithm version: .      <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:00:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to the only supported framework/algorithm version: .      \u001b]8;id=733528;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=119760;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\u001b\\\u001b[2m487\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:00:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:00:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=241663;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=80900;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\u001b\\\u001b[2m523\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor # type: ignore\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat # type: ignore\n",
    "\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=role_arn,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "## Check the baseline job in \"Amazon SageMaker > Processing jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:00:41] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         baseline-suggestion-job-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-02-00-38-911                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:00:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=836295;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=184596;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         baseline-suggestion-job-\u001b[1;36m2024\u001b[0m-11-25-02-00-38-911                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........2024-11-25 02:02:44.806930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-25 02:02:44.806963: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-25 02:02:46.537449: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-25 02:02:46.537481: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-25 02:02:46.537504: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-238-171.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2024-11-25 02:02:46.537782: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 02:02:48,257 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:807711953667:processing-job/baseline-suggestion-job-2024-11-25-02-00-38-911', 'ProcessingJobName': 'baseline-suggestion-job-2024-11-25-02-00-38-911', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-studio-807711953667-mmx0am1bt28/wine/data/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-807711953667/model-monitor/baselining/baseline-suggestion-job-2024-11-25-02-00-38-911/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::807711953667:role/service-role/AmazonSageMaker-ExecutionRole-20241121T213663', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\n",
      "2024-11-25 02:02:48,257 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\n",
      "2024-11-25 02:02:48,257 - __main__ - INFO - categorical_drift_method:None\n",
      "2024-11-25 02:02:48,257 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\n",
      "2024-11-25 02:02:48,257 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\n",
      "2024-11-25 02:02:48,257 - bootstrap - INFO - Copy aws jars\n",
      "2024-11-25 02:02:48,304 - bootstrap - INFO - Copy cluster config\n",
      "2024-11-25 02:02:48,305 - bootstrap - INFO - Write runtime cluster config\n",
      "2024-11-25 02:02:48,305 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\n",
      "2024-11-25 02:02:48,311 - bootstrap - INFO - Finished Yarn configuration files setup.\n",
      "2024-11-25 02:02:48,311 - bootstrap - INFO - Starting spark process for master node algo-1\n",
      "2024-11-25 02:02:48,311 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\n",
      "WARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\n",
      "2024-11-25 02:02:48,827 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = algo-1/10.0.238.171\n",
      "STARTUP_MSG:   args = [-format, -force]\n",
      "STARTUP_MSG:   version = 3.0.0\n",
      "STARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-\n",
      "nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\n",
      "STARTUP_MSG:   java = 1.8.0_392\n",
      "************************************************************/\n",
      "2024-11-25 02:02:48,835 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2024-11-25 02:02:48,839 INFO namenode.NameNode: createNameNode [-format, -force]\n",
      "Formatting using clusterid: CID-ac2e0644-86a4-4a3e-bc22-a56f083899a8\n",
      "2024-11-25 02:02:49,408 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2024-11-25 02:02:49,420 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2024-11-25 02:02:49,421 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2024-11-25 02:02:49,423 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2024-11-25 02:02:49,429 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
      "2024-11-25 02:02:49,429 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2024-11-25 02:02:49,429 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "2024-11-25 02:02:49,429 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2024-11-25 02:02:49,463 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2024-11-25 02:02:49,475 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2024-11-25 02:02:49,475 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2024-11-25 02:02:49,479 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2024-11-25 02:02:49,482 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Nov 25 02:02:49\n",
      "2024-11-25 02:02:49,484 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2024-11-25 02:02:49,484 INFO util.GSet: VM type       = 64-bit\n",
      "2024-11-25 02:02:49,486 INFO util.GSet: 2.0% max memory 3.0 GB = 61.4 MB\n",
      "2024-11-25 02:02:49,486 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
      "2024-11-25 02:02:49,523 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2024-11-25 02:02:49,527 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2024-11-25 02:02:49,527 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2024-11-25 02:02:49,555 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2024-11-25 02:02:49,555 INFO util.GSet: VM type       = 64-bit\n",
      "2024-11-25 02:02:49,556 INFO util.GSet: 1.0% max memory 3.0 GB = 30.7 MB\n",
      "2024-11-25 02:02:49,556 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2024-11-25 02:02:49,558 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2024-11-25 02:02:49,558 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2024-11-25 02:02:49,558 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2024-11-25 02:02:49,558 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2024-11-25 02:02:49,592 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\n",
      "2024-11-25 02:02:49,596 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2024-11-25 02:02:49,596 INFO util.GSet: VM type       = 64-bit\n",
      "2024-11-25 02:02:49,596 INFO util.GSet: 0.25% max memory 3.0 GB = 7.7 MB\n",
      "2024-11-25 02:02:49,596 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2024-11-25 02:02:49,604 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2024-11-25 02:02:49,604 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2024-11-25 02:02:49,604 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2024-11-25 02:02:49,607 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2024-11-25 02:02:49,607 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2024-11-25 02:02:49,609 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2024-11-25 02:02:49,609 INFO util.GSet: VM type       = 64-bit\n",
      "2024-11-25 02:02:49,609 INFO util.GSet: 0.029999999329447746% max memory 3.0 GB = 942.6 KB\n",
      "2024-11-25 02:02:49,609 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
      "2024-11-25 02:02:49,632 INFO namenode.FSImage: Allocated new BlockPoolId: BP-609463501-10.0.238.171-1732500169626\n",
      "2024-11-25 02:02:49,643 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\n",
      "2024-11-25 02:02:49,652 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2024-11-25 02:02:49,741 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\n",
      "2024-11-25 02:02:49,752 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2024-11-25 02:02:49,756 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.238.171\n",
      "************************************************************/\n",
      "2024-11-25 02:02:49,767 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\n",
      "2024-11-25 02:02:51,832 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\n",
      "2024-11-25 02:02:51,832 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\n",
      "2024-11-25 02:02:53,908 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\n",
      "2024-11-25 02:02:53,908 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "WARNING: /var/log/yarn/ does not exist. Creating.\n",
      "2024-11-25 02:02:56,007 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\n",
      "2024-11-25 02:02:56,007 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "2024-11-25 02:02:58,110 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\n",
      "2024-11-25 02:02:58,110 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "2024-11-25 02:03:00,441 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\n",
      "2024-11-25 02:03:00,442 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\n",
      "2024-11-25 02:03:10,449 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\n",
      "2024-11-25 02:03:12,314 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-11-25 02:03:12,735 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\n",
      "2024-11-25 02:03:12,776 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\n",
      "2024-11-25 02:03:12,787 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\n",
      "2024-11-25 02:03:13,370 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2024-11-25 02:03:13,410 INFO resource.ResourceUtils: ==============================================================\n",
      "2024-11-25 02:03:13,410 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2024-11-25 02:03:13,410 INFO resource.ResourceUtils: ==============================================================\n",
      "2024-11-25 02:03:13,411 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\n",
      "2024-11-25 02:03:13,439 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11384, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2024-11-25 02:03:13,454 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\n",
      "2024-11-25 02:03:13,456 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2024-11-25 02:03:13,514 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2024-11-25 02:03:13,514 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2024-11-25 02:03:13,515 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2024-11-25 02:03:13,515 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2024-11-25 02:03:13,515 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2024-11-25 02:03:13,859 INFO util.Utils: Successfully started service 'sparkDriver' on port 34647.\n",
      "2024-11-25 02:03:13,891 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2024-11-25 02:03:13,934 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2024-11-25 02:03:13,962 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2024-11-25 02:03:13,962 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2024-11-25 02:03:14,003 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2024-11-25 02:03:14,028 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-115295b3-10b0-473d-b23d-8312b3c9718c\n",
      "2024-11-25 02:03:14,049 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\n",
      "2024-11-25 02:03:14,091 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2024-11-25 02:03:14,129 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.238.171:34647/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1732500193365\n",
      "2024-11-25 02:03:14,669 INFO client.RMProxy: Connecting to ResourceManager at /10.0.238.171:8032\n",
      "2024-11-25 02:03:15,451 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-11-25 02:03:15,451 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-11-25 02:03:15,459 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15524 MB per container)\n",
      "2024-11-25 02:03:15,459 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2024-11-25 02:03:15,460 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2024-11-25 02:03:15,460 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2024-11-25 02:03:15,467 INFO yarn.Client: Preparing resources for our AM container\n",
      "2024-11-25 02:03:15,552 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2024-11-25 02:03:17,695 INFO yarn.Client: Uploading resource file:/tmp/spark-cdeb90f0-7df4-4414-a260-db31c9765607/__spark_libs__6658999518880238060.zip -> hdfs://10.0.238.171/user/root/.sparkStaging/application_1732500175655_0001/__spark_libs__6658999518880238060.zip\n",
      "2024-11-25 02:03:18,915 INFO yarn.Client: Uploading resource file:/tmp/spark-cdeb90f0-7df4-4414-a260-db31c9765607/__spark_conf__5578637336485676016.zip -> hdfs://10.0.238.171/user/root/.sparkStaging/application_1732500175655_0001/__spark_conf__.zip\n",
      "2024-11-25 02:03:19,366 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2024-11-25 02:03:19,366 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2024-11-25 02:03:19,366 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2024-11-25 02:03:19,366 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2024-11-25 02:03:19,366 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2024-11-25 02:03:19,399 INFO yarn.Client: Submitting application application_1732500175655_0001 to ResourceManager\n",
      "2024-11-25 02:03:19,609 INFO impl.YarnClientImpl: Submitted application application_1732500175655_0001\n",
      "2024-11-25 02:03:20,614 INFO yarn.Client: Application report for application_1732500175655_0001 (state: ACCEPTED)\n",
      "2024-11-25 02:03:20,618 INFO yarn.Client: \n",
      "#011 client token: N/A\n",
      "#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "#011 ApplicationMaster host: N/A\n",
      "#011 ApplicationMaster RPC port: -1\n",
      "#011 queue: default\n",
      "#011 start time: 1732500199505\n",
      "#011 final status: UNDEFINED\n",
      "#011 tracking URL: http://algo-1:8088/proxy/application_1732500175655_0001/\n",
      "#011 user: root\n",
      "2024-11-25 02:03:21,621 INFO yarn.Client: Application report for application_1732500175655_0001 (state: ACCEPTED)\n",
      "2024-11-25 02:03:22,624 INFO yarn.Client: Application report for application_1732500175655_0001 (state: ACCEPTED)\n",
      "2024-11-25 02:03:23,627 INFO yarn.Client: Application report for application_1732500175655_0001 (state: ACCEPTED)\n",
      "2024-11-25 02:03:24,639 INFO yarn.Client: Application report for application_1732500175655_0001 (state: ACCEPTED)\n",
      "2024-11-25 02:03:25,234 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1732500175655_0001), /proxy/application_1732500175655_0001\n",
      "2024-11-25 02:03:25,643 INFO yarn.Client: Application report for application_1732500175655_0001 (state: RUNNING)\n",
      "2024-11-25 02:03:25,647 INFO yarn.Client: \n",
      "#011 client token: N/A\n",
      "#011 diagnostics: N/A\n",
      "#011 ApplicationMaster host: 10.0.238.171\n",
      "#011 ApplicationMaster RPC port: -1\n",
      "#011 queue: default\n",
      "#011 start time: 1732500199505\n",
      "#011 final status: UNDEFINED\n",
      "#011 tracking URL: http://algo-1:8088/proxy/application_1732500175655_0001/\n",
      "#011 user: root\n",
      "2024-11-25 02:03:25,649 INFO cluster.YarnClientSchedulerBackend: Application application_1732500175655_0001 has started running.\n",
      "2024-11-25 02:03:25,682 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37963.\n",
      "2024-11-25 02:03:25,682 INFO netty.NettyBlockTransferService: Server created on 10.0.238.171:37963\n",
      "2024-11-25 02:03:25,685 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2024-11-25 02:03:25,696 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.238.171, 37963, None)\n",
      "2024-11-25 02:03:25,701 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.238.171:37963 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.238.171, 37963, None)\n",
      "2024-11-25 02:03:25,704 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.238.171, 37963, None)\n",
      "2024-11-25 02:03:25,706 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.238.171, 37963, None)\n",
      "2024-11-25 02:03:25,872 INFO util.log: Logging initialized @15172ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2024-11-25 02:03:26,754 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2024-11-25 02:03:30,235 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.238.171:47040) with ID 1,  ResourceProfileId 0\n",
      "2024-11-25 02:03:30,467 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:41233 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 41233, None)\n",
      "2024-11-25 02:03:44,544 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2024-11-25 02:03:44,759 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\n",
      "2024-11-25 02:03:44,821 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2024-11-25 02:03:44,827 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\n",
      "2024-11-25 02:03:45,991 INFO datasources.InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.\n",
      "2024-11-25 02:03:46,191 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\n",
      "2024-11-25 02:03:46,524 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\n",
      "2024-11-25 02:03:46,557 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.238.171:37963 (size: 39.2 KiB, free: 1458.6 MiB)\n",
      "2024-11-25 02:03:46,567 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\n",
      "2024-11-25 02:03:46,982 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-11-25 02:03:46,985 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-11-25 02:03:46,989 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 6093\n",
      "2024-11-25 02:03:47,047 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\n",
      "2024-11-25 02:03:47,067 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\n",
      "2024-11-25 02:03:47,068 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\n",
      "2024-11-25 02:03:47,068 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:03:47,070 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:47,076 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\n",
      "2024-11-25 02:03:47,130 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\n",
      "2024-11-25 02:03:47,134 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\n",
      "2024-11-25 02:03:47,135 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.238.171:37963 (size: 4.2 KiB, free: 1458.6 MiB)\n",
      "2024-11-25 02:03:47,136 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:47,155 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:47,156 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:47,201 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:47,449 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:41233 (size: 4.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:48,297 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:41233 (size: 39.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:48,647 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1459 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:48,649 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:48,657 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.549 s\n",
      "2024-11-25 02:03:48,661 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:03:48,662 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2024-11-25 02:03:48,664 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.616976 s\n",
      "2024-11-25 02:03:48,879 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:41233 in memory (size: 4.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:48,902 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.238.171:37963 in memory (size: 4.2 KiB, free: 1458.6 MiB)\n",
      "2024-11-25 02:03:51,273 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2024-11-25 02:03:51,274 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2024-11-25 02:03:51,277 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 12 more fields>\n",
      "2024-11-25 02:03:51,484 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:03:51,500 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:03:51,501 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.238.171:37963 (size: 39.2 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:03:51,502 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\n",
      "2024-11-25 02:03:51,520 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2024-11-25 02:03:51,569 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\n",
      "2024-11-25 02:03:51,571 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\n",
      "2024-11-25 02:03:51,572 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\n",
      "2024-11-25 02:03:51,572 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:03:51,576 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:51,582 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\n",
      "2024-11-25 02:03:51,677 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:03:51,686 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:03:51,690 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.238.171:37963 (size: 8.1 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:03:51,693 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:51,694 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:51,695 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:51,701 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:51,762 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:41233 (size: 8.1 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:52,813 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:41233 (size: 39.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:52,927 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:41233 (size: 10.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:53,075 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1377 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:53,075 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:53,076 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.490 s\n",
      "2024-11-25 02:03:53,077 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:03:53,077 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\n",
      "2024-11-25 02:03:53,078 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.507955 s\n",
      "2024-11-25 02:03:53,355 INFO codegen.CodeGenerator: Code generated in 188.594536 ms\n",
      "2024-11-25 02:03:53,510 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:41233 in memory (size: 8.1 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:53,515 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.238.171:37963 in memory (size: 8.1 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:03:53,967 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2024-11-25 02:03:54,124 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\n",
      "2024-11-25 02:03:54,133 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:03:54,134 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:03:54,134 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:03:54,137 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:54,139 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:03:54,166 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 114.3 KiB, free 1457.6 MiB)\n",
      "2024-11-25 02:03:54,168 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 1457.6 MiB)\n",
      "2024-11-25 02:03:54,169 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.238.171:37963 (size: 34.9 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:03:54,170 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:54,172 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:54,172 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:54,182 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:54,232 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:41233 (size: 34.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:55,611 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1431 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:55,612 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:55,614 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.471 s\n",
      "2024-11-25 02:03:55,614 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:03:55,615 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:03:55,615 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:03:55,616 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:03:55,731 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:03:55,734 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:03:55,734 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:03:55,734 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "2024-11-25 02:03:55,734 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:55,740 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:03:55,759 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.8 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:03:55,762 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:03:55,763 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.238.171:37963 (size: 45.9 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:03:55,764 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:55,766 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:55,766 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:55,771 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:55,800 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:41233 (size: 45.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:55,861 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.238.171:47040\n",
      "2024-11-25 02:03:56,340 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 571 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:56,340 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:56,341 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.591 s\n",
      "2024-11-25 02:03:56,342 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:03:56,342 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "2024-11-25 02:03:56,342 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.610928 s\n",
      "2024-11-25 02:03:56,401 INFO codegen.CodeGenerator: Code generated in 48.430555 ms\n",
      "2024-11-25 02:03:56,715 INFO codegen.CodeGenerator: Code generated in 33.301323 ms\n",
      "2024-11-25 02:03:56,797 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\n",
      "2024-11-25 02:03:56,799 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\n",
      "2024-11-25 02:03:56,799 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\n",
      "2024-11-25 02:03:56,799 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:03:56,801 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:56,802 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\n",
      "2024-11-25 02:03:56,828 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.2 KiB, free 1457.3 MiB)\n",
      "2024-11-25 02:03:56,830 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.3 MiB)\n",
      "2024-11-25 02:03:56,831 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.238.171:37963 (size: 16.4 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:03:56,832 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:56,833 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:56,834 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:56,836 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:56,857 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:41233 (size: 16.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:57,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 352 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:57,187 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:57,188 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.383 s\n",
      "2024-11-25 02:03:57,189 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:03:57,189 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\n",
      "2024-11-25 02:03:57,190 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.392406 s\n",
      "2024-11-25 02:03:57,726 INFO codegen.CodeGenerator: Code generated in 115.676628 ms\n",
      "2024-11-25 02:03:57,735 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\n",
      "2024-11-25 02:03:57,735 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:03:57,735 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:03:57,735 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:03:57,736 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:57,737 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:03:57,744 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 74.1 KiB, free 1457.2 MiB)\n",
      "2024-11-25 02:03:57,746 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1457.2 MiB)\n",
      "2024-11-25 02:03:57,747 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.238.171:37963 (size: 23.8 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:03:57,748 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:57,748 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:57,749 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:57,750 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:57,772 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41233 (size: 23.8 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:57,912 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 162 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:57,913 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:57,914 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.176 s\n",
      "2024-11-25 02:03:57,916 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:03:57,916 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:03:57,916 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:03:57,916 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:03:58,272 INFO codegen.CodeGenerator: Code generated in 155.861701 ms\n",
      "2024-11-25 02:03:58,285 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:03:58,287 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:03:58,287 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:03:58,288 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "2024-11-25 02:03:58,288 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:03:58,289 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:03:58,296 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:03:58,299 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:03:58,300 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.238.171:37963 (size: 19.4 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:03:58,301 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:58,301 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:58,302 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:58,303 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:58,321 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41233 (size: 19.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:03:58,331 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.238.171:47040\n",
      "2024-11-25 02:03:58,483 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 180 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:03:58,483 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:03:58,484 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.192 s\n",
      "2024-11-25 02:03:58,484 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:03:58,485 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\n",
      "2024-11-25 02:03:58,485 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.199393 s\n",
      "2024-11-25 02:03:58,644 INFO codegen.CodeGenerator: Code generated in 124.515439 ms\n",
      "2024-11-25 02:03:58,778 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\n",
      "2024-11-25 02:03:58,783 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\n",
      "2024-11-25 02:03:58,783 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\n",
      "2024-11-25 02:03:58,783 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\n",
      "2024-11-25 02:03:58,783 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "2024-11-25 02:03:58,783 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\n",
      "2024-11-25 02:03:58,786 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:03:58,797 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.5 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:03:58,799 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:03:58,800 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.238.171:37963 (size: 13.9 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:03:58,801 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:03:58,802 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:03:58,802 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:03:58,804 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:03:58,846 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41233 (size: 13.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:00,215 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1411 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:00,215 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:00,219 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.432 s\n",
      "2024-11-25 02:04:00,220 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:00,220 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:00,220 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\n",
      "2024-11-25 02:04:00,220 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:00,220 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:04:00,222 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:04:00,233 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:04:00,234 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.238.171:37963 (size: 3.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:00,234 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:00,235 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:00,235 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:00,238 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:00,252 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41233 (size: 3.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:00,261 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:00,310 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 73 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:00,311 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:00,312 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.090 s\n",
      "2024-11-25 02:04:00,312 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:00,312 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\n",
      "2024-11-25 02:04:00,312 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.533515 s\n",
      "2024-11-25 02:04:00,511 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\n",
      "2024-11-25 02:04:00,512 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:00,512 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:00,512 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:00,514 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:00,515 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:00,526 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 83.3 KiB, free 1457.0 MiB)\n",
      "2024-11-25 02:04:00,528 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.0 MiB)\n",
      "2024-11-25 02:04:00,529 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.238.171:37963 (size: 27.3 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:00,530 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:00,532 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:00,533 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:00,536 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:00,553 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41233 (size: 27.3 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:00,740 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 204 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:00,740 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:00,741 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.224 s\n",
      "2024-11-25 02:04:00,742 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:00,742 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:00,742 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:04:00,742 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:00,801 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:04:00,802 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:00,802 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:00,802 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "2024-11-25 02:04:00,803 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:00,804 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:00,812 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.9 KiB, free 1456.8 MiB)\n",
      "2024-11-25 02:04:00,815 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.0 KiB, free 1456.8 MiB)\n",
      "2024-11-25 02:04:00,816 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.238.171:37963 (size: 46.0 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:00,816 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:00,816 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:00,817 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:00,818 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:00,835 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:41233 (size: 46.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:00,847 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:00,967 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 149 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:00,967 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:00,968 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.163 s\n",
      "2024-11-25 02:04:00,968 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:00,968 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\n",
      "2024-11-25 02:04:00,969 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.167911 s\n",
      "2024-11-25 02:04:01,028 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:41233 in memory (size: 45.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,038 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.238.171:37963 in memory (size: 45.9 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:01,077 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.238.171:37963 in memory (size: 16.4 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,088 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:41233 in memory (size: 16.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,102 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.238.171:37963 in memory (size: 13.9 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,103 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:41233 in memory (size: 13.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,120 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41233 in memory (size: 19.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,125 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.238.171:37963 in memory (size: 19.4 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,145 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:41233 in memory (size: 23.8 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,149 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.238.171:37963 in memory (size: 23.8 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,160 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:41233 in memory (size: 27.3 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,178 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.238.171:37963 in memory (size: 27.3 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,187 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.238.171:37963 in memory (size: 3.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:01,191 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41233 in memory (size: 3.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,192 INFO codegen.CodeGenerator: Code generated in 40.504961 ms\n",
      "2024-11-25 02:04:01,210 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:41233 in memory (size: 34.9 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,212 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.238.171:37963 in memory (size: 34.9 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:01,231 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\n",
      "2024-11-25 02:04:01,250 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.238.171:37963 in memory (size: 46.0 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:01,251 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\n",
      "2024-11-25 02:04:01,251 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\n",
      "2024-11-25 02:04:01,251 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:01,252 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:01,254 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:41233 in memory (size: 46.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,262 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\n",
      "2024-11-25 02:04:01,282 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.1 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:04:01,285 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.7 MiB)\n",
      "2024-11-25 02:04:01,286 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.238.171:37963 (size: 16.3 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:01,286 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:01,287 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:01,287 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:01,291 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:01,308 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41233 (size: 16.3 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,380 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 89 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:01,381 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:01,383 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.119 s\n",
      "2024-11-25 02:04:01,383 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:01,384 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\n",
      "2024-11-25 02:04:01,384 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.152095 s\n",
      "2024-11-25 02:04:01,594 INFO codegen.CodeGenerator: Code generated in 59.776766 ms\n",
      "2024-11-25 02:04:01,603 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\n",
      "2024-11-25 02:04:01,603 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:01,604 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:01,604 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:01,605 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:01,605 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:01,609 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 73.6 KiB, free 1457.6 MiB)\n",
      "2024-11-25 02:04:01,611 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 1457.6 MiB)\n",
      "2024-11-25 02:04:01,612 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.238.171:37963 (size: 23.4 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:01,612 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:01,613 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:01,613 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:01,614 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:01,635 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:41233 (size: 23.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:01,712 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 98 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:01,712 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:01,718 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.111 s\n",
      "2024-11-25 02:04:01,718 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:01,718 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:01,718 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:04:01,718 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:02,007 INFO codegen.CodeGenerator: Code generated in 172.782839 ms\n",
      "2024-11-25 02:04:02,031 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:04:02,034 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:02,035 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:02,035 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "2024-11-25 02:04:02,035 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:02,035 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:02,038 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\n",
      "2024-11-25 02:04:02,040 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.5 MiB)\n",
      "2024-11-25 02:04:02,041 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.238.171:37963 (size: 19.2 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:02,043 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:02,043 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:02,044 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:02,046 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:02,066 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:41233 (size: 19.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:02,073 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:02,165 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 119 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:02,165 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:02,166 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.130 s\n",
      "2024-11-25 02:04:02,166 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:02,166 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\n",
      "2024-11-25 02:04:02,166 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.135121 s\n",
      "2024-11-25 02:04:02,275 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\n",
      "2024-11-25 02:04:02,277 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\n",
      "2024-11-25 02:04:02,278 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\n",
      "2024-11-25 02:04:02,279 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\n",
      "2024-11-25 02:04:02,279 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "2024-11-25 02:04:02,279 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\n",
      "2024-11-25 02:04:02,281 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:04:02,288 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 30.5 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:04:02,290 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:04:02,291 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.238.171:37963 (size: 14.0 KiB, free: 1458.5 MiB)\n",
      "2024-11-25 02:04:02,291 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:02,292 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:02,292 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:02,302 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:02,327 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:41233 (size: 14.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:02,384 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 83 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:02,384 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:02,386 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.103 s\n",
      "2024-11-25 02:04:02,387 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:02,387 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:02,388 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\n",
      "2024-11-25 02:04:02,388 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:02,388 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:04:02,391 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:04:02,393 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:04:02,394 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.238.171:37963 (size: 3.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:02,395 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:02,396 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:02,396 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:02,398 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:02,410 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:41233 (size: 3.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:02,415 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:02,502 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 104 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:02,503 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:02,505 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.114 s\n",
      "2024-11-25 02:04:02,506 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:02,506 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\n",
      "2024-11-25 02:04:02,507 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.231852 s\n",
      "2024-11-25 02:04:02,823 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\n",
      "2024-11-25 02:04:02,823 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:02,823 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:02,823 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:02,824 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:02,825 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:02,829 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 73.1 KiB, free 1457.4 MiB)\n",
      "2024-11-25 02:04:02,831 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.8 KiB, free 1457.3 MiB)\n",
      "2024-11-25 02:04:02,832 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.238.171:37963 (size: 25.8 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:02,833 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:02,834 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:02,834 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:02,835 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:02,847 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:41233 (size: 25.8 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:03,092 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 257 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:03,092 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:03,093 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.267 s\n",
      "2024-11-25 02:04:03,096 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:03,096 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:03,096 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:04:03,096 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:03,138 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:04:03,139 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:03,140 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:03,140 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
      "2024-11-25 02:04:03,140 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:03,142 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:03,149 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 141.6 KiB, free 1457.2 MiB)\n",
      "2024-11-25 02:04:03,151 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 1457.2 MiB)\n",
      "2024-11-25 02:04:03,152 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.238.171:37963 (size: 40.2 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:03,154 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:03,155 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:03,155 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:03,156 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:03,172 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:41233 (size: 40.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:03,200 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:03,352 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 196 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:03,353 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:03,354 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.210 s\n",
      "2024-11-25 02:04:03,354 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:03,354 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\n",
      "2024-11-25 02:04:03,355 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.216356 s\n",
      "2024-11-25 02:04:03,386 INFO codegen.CodeGenerator: Code generated in 28.119158 ms\n",
      "2024-11-25 02:04:03,488 INFO codegen.CodeGenerator: Code generated in 19.105136 ms\n",
      "2024-11-25 02:04:03,521 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\n",
      "2024-11-25 02:04:03,522 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\n",
      "2024-11-25 02:04:03,522 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\n",
      "2024-11-25 02:04:03,522 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:03,523 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:03,523 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\n",
      "2024-11-25 02:04:03,538 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 37.1 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:04:03,540 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1457.1 MiB)\n",
      "2024-11-25 02:04:03,541 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.238.171:37963 (size: 16.1 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:03,542 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:03,542 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:03,543 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:03,544 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:03,558 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:41233 (size: 16.1 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:03,654 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 110 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:03,654 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:03,655 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.131 s\n",
      "2024-11-25 02:04:03,656 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:03,656 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\n",
      "2024-11-25 02:04:03,657 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.136541 s\n",
      "2024-11-25 02:04:03,910 INFO codegen.CodeGenerator: Code generated in 55.721465 ms\n",
      "2024-11-25 02:04:03,919 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\n",
      "2024-11-25 02:04:03,919 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:03,919 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:03,919 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:03,920 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:03,920 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:03,925 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 63.3 KiB, free 1457.0 MiB)\n",
      "2024-11-25 02:04:03,927 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 1457.0 MiB)\n",
      "2024-11-25 02:04:03,928 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.238.171:37963 (size: 20.8 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:03,928 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:03,931 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:03,931 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:03,933 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:03,952 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:41233 (size: 20.8 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:04,012 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 79 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:04,013 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:04,014 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.093 s\n",
      "2024-11-25 02:04:04,014 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:04,014 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:04,014 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:04:04,015 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:04,110 INFO codegen.CodeGenerator: Code generated in 55.877708 ms\n",
      "2024-11-25 02:04:04,124 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\n",
      "2024-11-25 02:04:04,126 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\n",
      "2024-11-25 02:04:04,126 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\n",
      "2024-11-25 02:04:04,126 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\n",
      "2024-11-25 02:04:04,126 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:04,126 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\n",
      "2024-11-25 02:04:04,130 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 55.1 KiB, free 1457.0 MiB)\n",
      "2024-11-25 02:04:04,132 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,132 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.238.171:37963 (size: 16.7 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:04,133 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:04,133 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:04,133 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:04,135 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:04,155 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:41233 (size: 16.7 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:04,162 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:04,229 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 94 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:04,229 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:04,230 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.103 s\n",
      "2024-11-25 02:04:04,230 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:04,230 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\n",
      "2024-11-25 02:04:04,230 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.105810 s\n",
      "2024-11-25 02:04:04,318 INFO codegen.CodeGenerator: Code generated in 65.340172 ms\n",
      "2024-11-25 02:04:04,377 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\n",
      "2024-11-25 02:04:04,379 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\n",
      "2024-11-25 02:04:04,380 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\n",
      "2024-11-25 02:04:04,380 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\n",
      "2024-11-25 02:04:04,381 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "2024-11-25 02:04:04,381 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\n",
      "2024-11-25 02:04:04,384 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:04:04,389 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 30.5 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,392 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,393 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.238.171:37963 (size: 14.0 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:04,394 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:04,395 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:04,395 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:04,400 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:04,416 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:41233 (size: 14.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:04,486 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 86 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:04,486 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:04,487 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.101 s\n",
      "2024-11-25 02:04:04,487 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:04,487 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:04,487 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\n",
      "2024-11-25 02:04:04,487 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:04,488 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\n",
      "2024-11-25 02:04:04,490 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,492 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,493 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.238.171:37963 (size: 3.0 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:04,494 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:04,495 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:04,495 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:04,496 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:04,514 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:41233 (size: 3.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:04,520 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:04,551 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 55 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:04,551 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:04,552 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.063 s\n",
      "2024-11-25 02:04:04,552 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:04,552 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\n",
      "2024-11-25 02:04:04,553 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.175454 s\n",
      "2024-11-25 02:04:04,784 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\n",
      "2024-11-25 02:04:04,827 INFO codegen.CodeGenerator: Code generated in 10.195106 ms\n",
      "2024-11-25 02:04:04,839 INFO scheduler.DAGScheduler: Registering RDD 121 (count at StatsGenerator.scala:66) as input to shuffle 9\n",
      "2024-11-25 02:04:04,839 INFO scheduler.DAGScheduler: Got map stage job 20 (count at StatsGenerator.scala:66) with 1 output partitions\n",
      "2024-11-25 02:04:04,839 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (count at StatsGenerator.scala:66)\n",
      "2024-11-25 02:04:04,839 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2024-11-25 02:04:04,840 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:04,840 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66), which has no missing parents\n",
      "2024-11-25 02:04:04,844 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.5 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,850 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,851 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.238.171:37963 (size: 10.4 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:04,851 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:04,852 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:04,852 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:04,853 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:04,873 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:41233 (size: 10.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:04,931 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 78 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:04,931 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:04,932 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (count at StatsGenerator.scala:66) finished in 0.090 s\n",
      "2024-11-25 02:04:04,934 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2024-11-25 02:04:04,934 INFO scheduler.DAGScheduler: running: Set()\n",
      "2024-11-25 02:04:04,934 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2024-11-25 02:04:04,934 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2024-11-25 02:04:04,955 INFO codegen.CodeGenerator: Code generated in 13.111299 ms\n",
      "2024-11-25 02:04:04,970 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\n",
      "2024-11-25 02:04:04,971 INFO scheduler.DAGScheduler: Got job 21 (count at StatsGenerator.scala:66) with 1 output partitions\n",
      "2024-11-25 02:04:04,972 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at StatsGenerator.scala:66)\n",
      "2024-11-25 02:04:04,972 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "2024-11-25 02:04:04,972 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2024-11-25 02:04:04,972 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66), which has no missing parents\n",
      "2024-11-25 02:04:04,974 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 1456.9 MiB)\n",
      "2024-11-25 02:04:04,995 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1456.8 MiB)\n",
      "2024-11-25 02:04:04,997 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.238.171:37963 (size: 5.5 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:04,999 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\n",
      "2024-11-25 02:04:04,999 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\n",
      "2024-11-25 02:04:04,999 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "2024-11-25 02:04:05,001 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2024-11-25 02:04:05,001 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:41233 in memory (size: 20.8 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,004 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.238.171:37963 in memory (size: 20.8 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:05,022 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:41233 (size: 5.5 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,026 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.238.171:47040\n",
      "2024-11-25 02:04:05,032 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:41233 in memory (size: 23.4 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,054 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.238.171:37963 in memory (size: 23.4 KiB, free: 1458.3 MiB)\n",
      "2024-11-25 02:04:05,067 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 67 ms on algo-1 (executor 1) (1/1)\n",
      "2024-11-25 02:04:05,067 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "2024-11-25 02:04:05,068 INFO scheduler.DAGScheduler: ResultStage 31 (count at StatsGenerator.scala:66) finished in 0.095 s\n",
      "2024-11-25 02:04:05,068 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2024-11-25 02:04:05,069 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\n",
      "2024-11-25 02:04:05,069 INFO scheduler.DAGScheduler: Job 21 finished: count at StatsGenerator.scala:66, took 0.098483 s\n",
      "2024-11-25 02:04:05,154 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.238.171:37963 in memory (size: 14.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,167 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:41233 in memory (size: 14.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,221 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.238.171:37963 in memory (size: 16.7 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,230 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:41233 in memory (size: 16.7 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,260 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.238.171:37963 in memory (size: 3.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,269 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:41233 in memory (size: 3.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,321 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:41233 in memory (size: 14.0 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,340 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.238.171:37963 in memory (size: 14.0 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,386 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.238.171:37963 in memory (size: 19.2 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,399 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:41233 in memory (size: 19.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,435 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\n",
      "2024-11-25 02:04:05,451 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:41233 in memory (size: 40.2 KiB, free: 5.8 GiB)\n",
      "2024-11-25 02:04:05,460 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.238.171:37963 in memory (size: 40.2 KiB, free: 1458.4 MiB)\n",
      "2024-11-25 02:04:05,491 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "2024-11-25 02:04:05,528 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "2024-11-25 02:04:05,529 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "2024-11-25 02:04:05,538 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "2024-11-25 02:04:05,560 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2024-11-25 02:04:05,593 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2024-11-25 02:04:05,599 INFO storage.BlockManager: BlockManager stopped\n",
      "2024-11-25 02:04:05,606 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2024-11-25 02:04:05,612 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2024-11-25 02:04:05,641 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "2024-11-25 02:04:05,641 INFO Main: Completed: Job completed successfully with no violations.\n",
      "2024-11-25 02:04:05,641 INFO Main: Write to file /opt/ml/output/message.\n",
      "2024-11-25 02:04:05,646 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2024-11-25 02:04:05,646 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8baad995-d39e-47ec-a64b-5e3a462c8a46\n",
      "2024-11-25 02:04:05,652 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-cdeb90f0-7df4-4414-a260-db31c9765607\n",
      "2024-11-25 02:04:05,723 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\n",
      "2024-11-25 02:04:05,723 - DefaultDataAnalyzer - INFO - Spark job completed.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x24273451840>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset=f\"s3://{bucket}/wine/data/train.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    ")\n",
    "## 5m 24.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, provide the monitoring schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:07:32] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating Monitoring Schedule with name:                       <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#1560\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1560</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         wine-monitoring-schedule                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:07:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating Monitoring Schedule with name:                       \u001b]8;id=159074;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\u001b\\\u001b[2mmodel_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=963093;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#1560\u001b\\\u001b[2m1560\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         wine-monitoring-schedule                                      \u001b[2m                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "from sagemaker.model_monitor import CronExpressionGenerator # type: ignore\n",
    "\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='wine-monitoring-schedule',\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "## Go to \"Amazon SageMaker > Model dashboard > <your model> > Monitor schedule\" to check the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You can check that your schedule was created by ~~selecting the `SageMaker components and registries` tab on the far left~~. In this exercise you configured Model Monitor to watch a simple model. Next, we'll monitor the same deployment for explainability.\n",
    "\n",
    "* Go to `Amazon SageMaker > Model dashboard > <your model> > Monitor schedule`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ __REMINDER:__ Don't leave your model deployed overnight. If you aren't going to follow up with the Clarify exercise within a few hours, use the code below to remove your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete the monitors and endpoint\n",
    "# monitors = xgb_predictor.list_monitors()\n",
    "# for monitor in monitors:\n",
    "#     monitor.delete_monitoring_schedule()\n",
    "# xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **👉 Clarify**  \n",
    "\n",
    "For the last exercise we'll deploy an explainability monitor using [`Clarify`](https://aws.amazon.com/sagemaker/clarify/). We're going to use the model that you deployed in the last exercise, but if you cleaned up your deployments from the previous exercise, that's ok! You can rerun the deployment from the previous exercise up to the point where we deployed our model. It'll look like this:\n",
    "\n",
    "```python\n",
    "xgb_predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")\n",
    "```\n",
    "\n",
    "Once your model is deployed, you can come back here. _REMINDER_: you need to clean up your deployment, don't leave it running overnight. We'll provide some code at the end to delete your deployment.\n",
    "\n",
    "*  Amazon SageMaker Examples:    \n",
    "  [Fairness and Explainability with `SageMaker Clarify`](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-clarify/fairness_and_explainability/fairness_and_explainability.html)   \n",
    "\n",
    "## Prep\n",
    "\n",
    "We'll begin by reloading our data from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd # type: ignore\n",
    "# from sklearn import datasets # type: ignore\n",
    "# data = datasets.load_wine()\n",
    "# df = pd.DataFrame(data['data'])\n",
    "# df.columns = data['feature_names']\n",
    "# df.rename(columns = {'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to put the target variable in the first column per the docs for our chosen algorithm: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"TARGET\"] = data['target']\n",
    "# df.set_index(df.pop('TARGET'), inplace=True)\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the data to S3 as train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# from io import BytesIO #StringIO\n",
    "\n",
    "# s3_client = boto3.client('s3')\n",
    "# delimiter = int(len(df)/2)\n",
    "# df_train, df_val = df.iloc[delimiter:], df.iloc[:delimiter]\n",
    "\n",
    "# ## prepare training data \n",
    "# csv_buffer = BytesIO()\n",
    "# df_train.to_csv(csv_buffer, header=False, index=False)  ## send to buffer\n",
    "# csv_buffer.seek(0)\n",
    "# s3_key = \"wine/data/train.csv\"  \n",
    "# s3_client.put_object(Body=csv_buffer, Bucket=bucket, Key=s3_key)  ## uplaod to S3\n",
    "# train_input = sagemaker.inputs.TrainingInput(\n",
    "#     s3_data=f\"s3://{bucket}/{s3_key}\", \n",
    "#     content_type='csv')\n",
    "# print(f\"s3://{bucket}/{s3_key}\") \n",
    "\n",
    "# ## prepare validation data\n",
    "# csv_buffer = BytesIO()\n",
    "# df_val.to_csv(csv_buffer, header=False, index=False)  ## send to buffer\n",
    "# csv_buffer.seek(0)\n",
    "# s3_key = \"wine/data/validation.csv\" \n",
    "# s3_client.put_object(Body=csv_buffer, Bucket=bucket, Key=s3_key)  ## upload to S3\n",
    "# val_input = sagemaker.inputs.TrainingInput(\n",
    "#     s3_data=f\"s3://{bucket}/{s3_key}\", \n",
    "#     content_type='csv')\n",
    "# print(f\"s3://{bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our data is staged and our model is deployed - let's monitor it for explainability. We need to define three config objects, the `SHAPConfig`, the `ModelConfig`, and the `ExplainabilityAnalysisConfig`. Below, we provide the `SHAPConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    baseline=[df_train.mean().astype(int).to_list()[1:]],\n",
    "    num_samples=int(df_train.size),\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, fill in the blanks to define the `ModelConfig` and `ExplainabilityAnalysisConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=xgb_predictor.endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"text/csv\",\n",
    "    accept_type=\"text/csv\",\n",
    ")\n",
    "analysis_config = sagemaker.model_monitor.ExplainabilityAnalysisConfig(\n",
    "    explainability_config=shap_config,\n",
    "    model_config=model_config,\n",
    "    headers=df_train.columns.to_list()[1:],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply our config, we need to create the monitor object. This is what we'll apply all our config to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:24:03] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to the only supported framework/algorithm version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>.   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:24:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to the only supported framework/algorithm version: \u001b[1;36m1.0\u001b[0m.   \u001b]8;id=714777;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=130886;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\u001b\\\u001b[2m487\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:24:04] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:24:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=62379;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=210769;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\u001b\\\u001b[2m523\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_explainability_monitor = (\n",
    "    sagemaker.model_monitor.ModelExplainabilityMonitor(\n",
    "        role=role_arn,\n",
    "        sagemaker_session=session,\n",
    "        max_runtime_in_seconds=1800)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything's ready! Below, create a monitoring schedule using the configs we created. Set the schedule to run _daily_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:24:06] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Uploading analysis config to <span style=\"font-weight: bold\">{</span>s3_uri<span style=\"font-weight: bold\">}</span>.                 <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clarify_model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py#227\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:24:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Uploading analysis config to \u001b[1m{\u001b[0ms3_uri\u001b[1m}\u001b[0m.                 \u001b]8;id=236711;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py\u001b\\\u001b[2mclarify_model_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666900;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py#227\u001b\\\u001b[2m227\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:24:08] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating Monitoring Schedule with name:                       <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#1560\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1560</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         monitoring-schedule-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-02-24-06-231                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:24:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating Monitoring Schedule with name:                       \u001b]8;id=223458;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\u001b\\\u001b[2mmodel_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=373912;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#1560\u001b\\\u001b[2m1560\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         monitoring-schedule-\u001b[1;36m2024\u001b[0m-11-25-02-24-06-231                   \u001b[2m                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO \n",
    "from sagemaker.model_monitor import CronExpressionGenerator  # type: ignore\n",
    "\n",
    "explainability_uri = f\"s3://{bucket}/wine/model_explainability\"\n",
    "model_explainability_monitor.create_monitoring_schedule(\n",
    "    output_s3_uri=explainability_uri,\n",
    "    analysis_config=analysis_config,\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.daily() #.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to go! You can check that your schedule was created by selecting ~~the `SageMaker components and registries` tab on the far left~~. In this exercise you deployed a monitor for explainability to your SageMaker endpoint. This is the last exercise - you'll apply these learnings again in your Project at the end of the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ __REMINDER:__ Don't leave your model deployed overnight. Use the code below to remove your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:12] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to the only supported framework/algorithm version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>.   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to the only supported framework/algorithm version: \u001b[1;36m1.0\u001b[0m.   \u001b]8;id=558324;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=622303;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\u001b\\\u001b[2m487\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=110601;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=638033;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\u001b\\\u001b[2m523\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:13] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to the only supported framework/algorithm version: .      <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to the only supported framework/algorithm version: .      \u001b]8;id=773670;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=229123;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#487\u001b\\\u001b[2m487\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=693648;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=484323;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\image_uris.py#523\u001b\\\u001b[2m523\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting Monitoring Schedule with name:                                <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#2237\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2237</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         monitoring-schedule-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-02-24-06-231                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting Monitoring Schedule with name:                                \u001b]8;id=877498;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=926090;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#2237\u001b\\\u001b[2m2237\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         monitoring-schedule-\u001b[1;36m2024\u001b[0m-11-25-02-24-06-231                            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:19] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting Model Explainability Job Definition with     <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clarify_model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py#1289\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1289</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         name:                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         model-explainability-job-definition-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-02-24- <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         06-231                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting Model Explainability Job Definition with     \u001b]8;id=150250;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py\u001b\\\u001b[2mclarify_model_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=541800;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\clarify_model_monitoring.py#1289\u001b\\\u001b[2m1289\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         name:                                                 \u001b[2m                                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         model-explainability-job-definition-\u001b[1;36m2024\u001b[0m-11-25-02-24- \u001b[2m                                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         06-231                                                \u001b[2m                                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:20] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting Monitoring Schedule with name: wine-monitoring-schedule       <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#2237\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2237</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting Monitoring Schedule with name: wine-monitoring-schedule       \u001b]8;id=169121;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=922755;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#2237\u001b\\\u001b[2m2237\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting Data Quality Job Definition with name:               <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#2456\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2456</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         data-quality-job-definition-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-02-07-31-428           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting Data Quality Job Definition with name:               \u001b]8;id=925298;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py\u001b\\\u001b[2mmodel_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=685787;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\model_monitor\\model_monitoring.py#2456\u001b\\\u001b[2m2456\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         data-quality-job-definition-\u001b[1;36m2024\u001b[0m-11-25-02-07-31-428           \u001b[2m                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:31] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4796\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4796</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-48-48-543                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=206393;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=458157;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4796\u001b\\\u001b[2m4796\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-48-48-543                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/24/24 20:32:32] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name: xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-25-01-48-48-543           <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4786\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4786</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/24/24 20:32:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name: xgboost-\u001b[1;36m2024\u001b[0m-11-25-01-48-48-543           \u001b]8;id=995407;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=372637;file://d:\\Users\\guido\\miniconda3\\envs\\awsmle_py310\\lib\\site-packages\\sagemaker\\session.py#4786\u001b\\\u001b[2m4786\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## delete all the monitors and endpoints\n",
    "monitors = xgb_predictor.list_monitors()\n",
    "for monitor in monitors:\n",
    "    monitor.delete_monitoring_schedule()\n",
    "xgb_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "awsmle_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
